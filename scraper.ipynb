{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium==4.24.0 in /home/ee303/.local/lib/python3.11/site-packages (4.24.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /home/ee303/anaconda3/lib/python3.11/site-packages (from selenium==4.24.0) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in /home/ee303/.local/lib/python3.11/site-packages (from selenium==4.24.0) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/ee303/.local/lib/python3.11/site-packages (from selenium==4.24.0) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/ee303/anaconda3/lib/python3.11/site-packages (from selenium==4.24.0) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/ee303/.local/lib/python3.11/site-packages (from selenium==4.24.0) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/ee303/.local/lib/python3.11/site-packages (from selenium==4.24.0) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/ee303/.local/lib/python3.11/site-packages (from trio~=0.17->selenium==4.24.0) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/ee303/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium==4.24.0) (2.4.0)\n",
      "Requirement already satisfied: idna in /home/ee303/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium==4.24.0) (3.4)\n",
      "Requirement already satisfied: outcome in /home/ee303/.local/lib/python3.11/site-packages (from trio~=0.17->selenium==4.24.0) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/ee303/.local/lib/python3.11/site-packages (from trio~=0.17->selenium==4.24.0) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/ee303/.local/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium==4.24.0) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/ee303/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium==4.24.0) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/ee303/.local/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium==4.24.0) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium==4.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ee303/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query source and output result path\n",
    "student_id = \"M11307510\"\n",
    "query_path = f\"./queries/{student_id}_queries.txt\"\n",
    "results_path = \"./results\"\n",
    "\n",
    "# Web scraping target URL\n",
    "search_url = \"https://www.tw.coupang.com/search?q=\"\n",
    "\n",
    "# Scraping parameter settings\n",
    "short_time_sleep = 1\n",
    "medium_time_sleep = 3\n",
    "long_time_sleep = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries from file\n",
    "def read_queries(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "    return lines\n",
    "\n",
    "# Check if the webpage is accessible\n",
    "def check_access(driver):\n",
    "    # Check if the page is accessible\n",
    "    denied = driver.find_elements(By.XPATH, '/html/body/h1')\n",
    "    if denied:\n",
    "        if denied[0].text == 'Access Denied':\n",
    "            return False\n",
    "        else: \n",
    "            return True\n",
    "    # Check if the page has results\n",
    "    no_result = driver.find_elements(By.XPATH, '/html/body/div[1]/div/main/div/div/div[3]/div[3]')\n",
    "    if no_result:\n",
    "        if no_result[0].text == '無相關搜索結果':\n",
    "            return False\n",
    "        else: \n",
    "            return True\n",
    "    return True\n",
    "\n",
    "# Scroll the webpage to the bottom\n",
    "def scroll_to_bottom(driver, pause_time=3):\n",
    "    # Get the current scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # Scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Pause to allow any new content to load\n",
    "    time.sleep(pause_time)\n",
    "    \n",
    "    # Get the new scroll height after scrolling\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # If the new height is greater than the last height, new content has loaded\n",
    "    if not (new_height > last_height):\n",
    "        driver.execute_script(\"window.scrollBy(0, -100);\")\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "\n",
    "# Get all items from pages   \n",
    "def get_all_items(driver):\n",
    "    try:\n",
    "        items = driver.find_elements(By.XPATH, '/html/body/div[1]/div[2]/main/div/div/div[3]/div/div/a')\n",
    "        return items\n",
    "    except:\n",
    "        print(\"No items found.\")\n",
    "\n",
    "# Extract all item informations to a dataframe   \n",
    "def extract_item_info(items):\n",
    "    print(\"Extracting item information...\")\n",
    "    data = []\n",
    "    for i, item in enumerate(items):\n",
    "        try:\n",
    "            item_name = item.find_element(By.XPATH, 'div[2]').text\n",
    "            item_url = item.get_attribute('href')\n",
    "            # price is in one of two possible XPaths\n",
    "            try:\n",
    "                item_price = item.find_element(By.XPATH, 'div[3]/div[2]/span/span').text # on sale.\n",
    "            except:\n",
    "                try:\n",
    "                    item_price = item.find_element(By.XPATH, 'div[3]/div[1]/span/span').text # not on sale.\n",
    "                except:\n",
    "                    item_price = \"null\"\n",
    "            \n",
    "            data.append({\n",
    "                'product_name': item_name,\n",
    "                'product_price': item_price,\n",
    "                'product_url': item_url\n",
    "            })\n",
    "        except:\n",
    "            print(f\"Error extracting item {i}.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def start_browser_with_proxy(proxy_ip):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # 使用 ChromeOptions 添加代理設置\n",
    "    chrome_options.add_argument(f'--proxy-server={proxy_ip}')\n",
    "    \n",
    "    # 啟動 Chrome 瀏覽器並應用代理\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Chrome options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['178.128.113.118:23128', '43.153.207.93:3128', '154.94.5.241:7001', '72.10.160.92:5635', '160.86.242.23:8080', '211.104.20.205:8080', '72.10.160.170:2657', '51.83.62.245:8080', '4.159.28.85:8080', '72.10.160.173:29439', '35.220.254.137:8080', '67.43.227.230:4961', '104.248.98.31:3128', '4.158.237.61:8080', '43.153.237.252:3128', '58.240.211.250:7890', '67.43.228.250:26991', '51.178.149.106:8080', '202.188.211.11:800', '45.119.133.218:3128', '4.159.29.241:8080', '8.148.23.202:4006', '67.43.227.229:16401', '8.219.97.248:80', '67.43.236.21:8307', '72.10.160.174:13093', '72.10.160.90:1365', '13.87.97.69:8080', '114.129.2.82:8081', '87.247.186.40:1081', '103.166.8.228:1080', '171.244.60.55:8080', '5.202.149.241:8080', '4.158.2.131:8080', '43.134.1.40:3128', '67.43.236.18:1853', '36.72.245.209:8080', '47.243.92.199:3128', '47.89.184.18:3128', '8.213.151.128:3128', '20.44.188.17:3129', '148.64.110.245:3129', '20.44.189.184:3129', '20.204.214.23:3129', '20.204.214.79:3129', '122.152.4.135:6000', '67.43.236.20:10145', '140.227.119.176:3128', '140.227.119.162:3128', '176.32.2.193:8080', '140.227.119.140:3128', '200.63.107.118:8089', '164.70.117.168:3128', '43.134.229.98:3128', '193.187.172.82:3128', '200.37.187.59:999', '86.109.3.28:10013', '37.60.255.91:3128', '62.201.251.217:8585', '79.143.177.29:21972', '144.86.187.34:3129', '47.237.113.119:3128', '103.209.38.132:81', '101.255.123.18:8181', '103.165.218.234:8085', '5.161.114.204:4228', '45.224.151.253:999', '148.255.124.220:999', '182.253.122.156:8080', '199.195.253.213:3128', '194.164.206.37:3128', '177.93.45.225:999', '103.18.77.50:1111', '47.91.65.23:3128', '27.147.218.162:8080', '43.247.33.60:8080', '185.208.101.216:8080', '118.117.188.129:8089', '151.232.177.134:8080', '106.227.95.142:3129', '47.237.2.245:3128', '103.148.29.229:80', '43.153.208.148:3128', '93.190.14.204:8080', '47.251.43.115:33333', '34.92.88.81:33333', '178.48.68.61:18080', '188.166.197.129:3128', '52.82.123.144:3128', '119.96.100.63:30000', '142.171.90.93:3128', '178.254.42.100:8118', '115.79.27.106:1001', '41.65.0.221:1976', '93.125.3.22:8080', '45.175.252.18:999', '103.165.155.254:2016', '51.79.71.106:8080', '47.122.65.254:3128', '47.122.5.165:3128']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    " \n",
    " \n",
    "response = requests.get(\"https://www.sslproxies.org/\")\n",
    "proxy_ips = re.findall('\\d+\\.\\d+\\.\\d+\\.\\d+:\\d+', response.text)  #「\\d+」代表數字一個位數以上\n",
    "print(proxy_ips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start web scraping.\n",
    "#### (During scraping, you may open other windows, but do not close or minimize the Chrome window that is performing the scraping.)\n",
    "#### (Make sure the screen remains on while the scraper is running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use IP:[20.204.214.79:3129]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "start_browser_with_proxy() missing 1 required positional argument: 'proxy_ip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m ip\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(proxy_ips)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse IP:[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mip\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m driver \u001b[38;5;241m=\u001b[39m start_browser_with_proxy()\n\u001b[1;32m      8\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(short_time_sleep)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Get all csv files in the results folder, if exists pass the query\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: start_browser_with_proxy() missing 1 required positional argument: 'proxy_ip'"
     ]
    }
   ],
   "source": [
    "queries = read_queries(query_path)\n",
    "\n",
    "for query in queries:\n",
    "    # 確保每次使用新的代理 IP\n",
    "    ip = random.choice(proxy_ips)\n",
    "    print(f\"Use IP: [{ip}]\")\n",
    "    \n",
    "    driver = start_browser_with_proxy(ip)\n",
    "    time.sleep(short_time_sleep)\n",
    "\n",
    "    # 檢查結果是否已存在\n",
    "    csv_files = [f for f in os.listdir(results_path) if f.endswith('.csv')]\n",
    "    search_string = query\n",
    "    all_contain_string = any(search_string in file_name for file_name in csv_files)\n",
    "    if all_contain_string:\n",
    "        print(f\"Results for {query} have already been scraped. Skipping...\\n\")\n",
    "        driver.quit()\n",
    "        continue\n",
    "\n",
    "    # 搜索查詢\n",
    "    try:\n",
    "        driver.get(search_url + query)\n",
    "        time.sleep(medium_time_sleep)\n",
    "        status = check_access(driver)\n",
    "        if status:\n",
    "            print(f\"Start scraping {query}...\")\n",
    "        else:\n",
    "            print(f\"Some error occurred while scraping {query}.\")\n",
    "            driver.quit()\n",
    "            continue\n",
    "        \n",
    "        while status:\n",
    "            status = check_access(driver)\n",
    "            if status:  # Only proceed to scroll if check_access is True\n",
    "                status = scroll_to_bottom(driver, medium_time_sleep + random.random())  # add time noise\n",
    "\n",
    "        # 處理項目\n",
    "        items = get_all_items(driver)\n",
    "        items_df = extract_item_info(items)\n",
    "\n",
    "        # 保存結果到 CSV 文件\n",
    "        file_path = os.path.join(results_path, f\"{student_id}_{query}.csv\")\n",
    "        items_df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Results for {query} have been saved to {file_path}\")\n",
    "\n",
    "    finally:\n",
    "        # 關閉瀏覽器\n",
    "        driver.quit()\n",
    "    \n",
    "    # 隨機延遲，避免被封\n",
    "    time.sleep(long_time_sleep + random.random() * 10)\n",
    "    print(\"Sleeping for a while...\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
